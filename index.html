<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Visualization and analysis of probability   distributions of large temporal data</title>
    <meta charset="utf-8" />
    <link href="index_files/remark-css/ninjutsu.css" rel="stylesheet" />
    <link rel="stylesheet" href="assets/animate.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-logo.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-brand.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets/styles.css" type="text/css" />
    <link rel="stylesheet" href="assets/custom.css" type="text/css" />
    <link rel="stylesheet" href="timeline.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Visualization and analysis of probability <br> distributions of large temporal data
### <br> Sayani Gupta <br>
<hr>
  Prof. Rob J Hyndman     Prof. Dianne Cook     Dr. Peter Toscas
### <font size="5"> Department of Econometrics and Business Statistics <br> Pre-submission review <br> March 16, 2021

---











background-image: url("figs/motivation2.png")

background-position: center
background-size: contain

---
# Time series of electricity demand for different households



&lt;img src="figure/elec-rawnew-1.svg" width="90%" style="display: block; margin: auto;" /&gt;


.center[
.card[.smaller[fine time resolutions]]
.card[.smaller[many measurements]]
.card[.smaller[no behavior visible in the squeezed linear view]]  
.card[.smaller[many households]]
.card[.smaller[unequal length]]
.card[.smaller[different start and end date]]
.card[.smaller[missing observations]]
]




---

class:top, center

# Research theme and roadmap

.left-column[
- fine resolution data could be analyzed at any coarser temporal scales
- application beyond smart meters
- solutions to be developed for any temporal data observed more than once per year
]
.right-column[
&lt;img src="figs/theme-roadmap2.png" width="80%" style="display: block; margin: auto;" /&gt;
]

???

thisd is mynotes 
---

class:left, top

#  Papers

- Visualizing probability distributions across bivariate cyclic temporal granularities (tentatively accepted in JCGS in March, 2021) &lt;br&gt;&lt;br&gt;
- A new metric for automatic discovery of periodic patterns in time series (draft prepared and incorporating comments from supervisors) &lt;br&gt;&lt;br&gt;
- Clustering of residential Australian smart meter customers (draft in preparation)

---


class: middle center

&lt;!-- SLIDE 4 --&gt;

.animated.bounce[
&lt;img src="images/gravitas_sticker.png" height=280px&gt;
]

## Visualizing probability distributions across bivariate cyclic temporal granularities



---

# Project 1: Main contributions

 - provide a formal characterization of cyclic granularities;
 - facilitate manipulation of single- and multiple-order-up time granularities through cyclic calendar algebra;
 - develop an approach to check the feasibility of creating plots for any two cyclic granularities.

---
class: top left 

# Project 1: Linear to cyclic

.pull-left[
.checked[
.smaller[
- **Cyclic time granularities:** exploring different periodicities e.g. hour-of-day, day-of-month or  hour-of-week, day-of-semester
- **Multiple observations** for each level of cyclic granularity
- **summarize distribution** of measured variables
]
]
]

.pull-right[
&lt;img src="figure/linear2cyclic-1.png" style="display: block; margin: auto;" /&gt;&lt;img src="figure/linear2cyclic-2.png" style="display: block; margin: auto;" /&gt;
]

---

# Project 1: Graphing distribution summary


&lt;img src="figs/allplots.png" width="4453" style="display: block; margin: auto;" /&gt;


**_several ways to summarize a distribution_**

---
class: top left

# Project 1: Data structure and graphical mapping

.left-column[
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
.smaller[
- extension of tsibble data structure
- Many possible cyclic granularities (e.g. _hod_ read as hour-of-day, _dow_ day-of-week and so on)
- choose any two `\(C_i = \{A_1, A_2, \dots, A_K\}\)` and `\(C_j = \{B_1, B_2, \dots, B_L\}\)`
- graphical mapping `\((C_i, C_j, v)\)`
- `\(^{N_C}P_2\)` displays
- Too many for large `\({N_C}\)`
]
]

.right-column[

&lt;img src="figs/data-structure2.png" width="90%" style="display: block; margin: auto;" /&gt;

]
---
class:left, top

# Project 1: Relationship between cyclic granularities




&lt;img src="figure/bothclash2-1.svg" width="80%" style="display: block; margin: auto;" /&gt;

.smaller[
.card[_Clashes_: pairs leading to empty sets]
.card[_Harmonies_: pairs leading to no empty sets]  
.smaller[
_1.Still too many harmonies to display for large `\(N_C\)`_  
_2.**Significant harmonies:** The harmonies along which patterns are significant enough to be an interesting display_
]
]
---
class: top left

# Show me significant harmonies only

&lt;!-- SLIDE 4 --&gt;
&lt;br&gt;
&lt;br&gt;
##  Project 2: A new metric for automatic discovery of periodic patterns in time series

---

# Project 2: Contributions

 - introduce a new distance measure for quantifying periodic interactions;
 - devise a framework for choosing a threshold for detecting only significantly interesting periodic patterns;
 - show that the proposed distance metric is useful for ranking different periodic patterns since they have been normalized for relevant parameters.

---

# Project 2: Idea

.left-column[
&lt;br&gt;
- Sample designs
- Rank harmonies `\(a&lt;b&lt;c&lt;d\)`
- Gestalt theory
- distance to compute within and between facet variation
- define a threshold for significance
]

.right-column[
&lt;img src="figs/example-design.png" width="90%" style="display: block; margin: auto;" /&gt;
]

.smaller[
**Designs:** 
`\(a \gets D_{null}\)`, `\(b\gets D_{var_f}\)`,  `\(c\gets D_{var_x}\)`, `\(d \gets D_{var_{all}}\)`
]
---

# Project 2: Computation

## Distance between distributions Jensen-Shannon distance

`$$JSD(q_1||q_2) = \frac{1}{2}D(q_1||M) + \frac{1}{2}D(q_2||M)$$`
where `\(M = \frac{q_1+q_2}{2}\)` and 
`\(D(q_1||q_2) := \int^\infty_{-\infty} q_1(x)f(\frac{q_1(x)}{q_2(x)})\)` is the KL divergence between distributions `\(q_1\)` and `\(q_2\)`.  


.smaller[
.smaller[
.smaller[
.footnote[
Menéndez, M. L., J. A. Pardo, L. Pardo, and M. C. Pardo. 1997. “The Jensen-Shannon Divergence.” Journal of the Franklin Institute 334 (2): 307–18.
]
]
]
]
---

# Project 2: Computation (continued)

# Within and between facet distances

&lt;img src="figs/dist_explain.png" width="2331" style="display: block; margin: auto;" /&gt;

.smaller[
- two cyclic granularities `\(A\)` and `\(B\)` placed across x-axis and facet respectively. 
- `\(A = \{ a_j: j = 1, 2, \dots, J\}\)` and `\(B = \{ b_k: k = 1, 2, \dots, K\}\)`
- within-facet distances `\((a_{j}b_{k}, a_{j'}b_{k})\)`
- between-facet distances `\((a_{j}b_{k}, a_{j}b_{k'})\)`
]

---

# Project 2: Computation (continued)

## Data pre-processing on asymmetrical distributed real world observed variables

.smaller[
The empirical **Normal Quantile Transform** involves the following steps:

  1. 1) Sort the sample of measured variable `\(X\)` from the smallest to the largest observation `\(x_{(1)},\dots, x_{(i)}, .., x_{(n)}\)`.
  2. 2) Estimate the cumulative probabilities `\(p_{(1)},\dots, p_{(i)}, .., p_{(n)}\)` where `\(p_{(i)} = P(X\leq x_{(i)}) = \frac{i}{n+1}\)`.
  3. 3) Transform each observation `\(x_{(i)}\)` of `\(X\)` into observation `\(y_{(i)} = Q^{-1}(p(i))\)` of the standard normal variate `\(Y\)` , with `\(Q\)` denoting the standard normal distribution and `\(Q^{-1}\)` its inverse.
]

.smaller[
.smaller[
.smaller[
.footnote[
Bogner, K., F. Pappenberger, and H. L. Cloke. 2012. “Technical Note: The Normal Quantile Transformation and Its Application in a Flood Forecasting System.” Hydrology and Earth System Sciences 16 (4): 1085–94.
]
]
]
]
---

# Project 2: Computation (continued)

## Algorithm for weighted pairwise distances (wpd)

&lt;img src="figs/algorithm_revise.png" width="80%" style="display: block; margin: auto;" /&gt;
---

# Project 2: Simulation setup

.smaller[
- Design correspond to `\(D_{null}\)`
- `\(nx = nfacet =  \{2, 3, 5, 7, 14, 20, 31, 50\}\)`  
- Observations are generated from a Gamma(2,1) distribution for each combination of `\(nx\)` and `\(nfacet\)`
- `\(ntimes = 500\)` observations are drawn for each combination of the categories
- For eg, for the panel with `\(\{nx = 2, nfacet = 2\}\)`, `\(500\)` observations are generated for each of `\(\{(1, 1), (1, 2), (2, 1), (2, 2)\}\)`
- Data is simulated for each of the panels `\(nsim=200\)` times
- `\(wpd\)` computed for each of the simulation and panel for `\(\lambda =  0.67\)`
]

&lt;!--  _ **nx** (number of x-axis categories), **nfacet** (number of facet categories), **nsim** (number of simulations to draw the distribution, **ntimes** (number of observations per combination), `\(\lambda\)` (value of tuning parameter)_ --&gt;

&lt;!-- ### Values --&gt;

&lt;!-- `\(nx = \{2, 3, 5, 7, 14, 20, 31, 50\}\)`   --&gt;
&lt;!-- `\(nfacet = \{2, 3, 5, 7, 14, 20, 31, 50\}\)`   --&gt;
&lt;!-- `\(nsim=200\)`   --&gt;
&lt;!-- `\(ntimes = 500\)`   --&gt;
&lt;!-- `\(\lambda =  0.67\)`   --&gt;


---

# Project 2: Null distribution of `\(wpd\)` 

&lt;img src="figure/raw-dist-1.svg" style="display: block; margin: auto;" /&gt;

**_Distribution changes across different comparisons_**
---



# Project 2: Need to normalize

&lt;br&gt;

`\(wpd\)` values are meant to be different only when there is difference between different categories and not when 
&lt;br&gt;

- underlying distributions are different
(solved by using NQT)
- number of categories are different
(solved by normalizing for the number of categories)

---

# Project 2: Normalization approach (Permutation)

&lt;img src="figure/permutation-test-1.svg" style="display: block; margin: auto;" /&gt;
.smaller[
.smaller[
- `\(wpd_{orig}\)` : `\(wpd\)` computed for original data
- `\(wpd_{perm_i}\)`: `\(wpd\)` computed for `\(i^{th}\)` permutation of data
- Repeated for many (200) permutations
- `\(wpd_{norm} =  \frac{(wpd_{orig} - \bar{wpd_{perm}})}{\sigma(wpd_{perm})}\)`
**_Computationally expensive_**

]
]


---
# Project 2: Simulation results (Permutation)

&lt;img src="figure/perm-dist-1.svg" width="90%" style="display: block; margin: auto;" /&gt;

.smaller[
**_Location and scale of distributions look similar for different comparisons_**
]
---
# Project 2: Normalization approach (GLM)

.pull-left[

.smaller[
`\(y = a+b*log(z) + \epsilon \quad \epsilon \sim IID\)`
]

.smaller[
.smaller[
- Response: `\(y = median(wpd)\)`
- Linear predictor: `\(z = log(nx*nfacet)\)`
- Distribution: Gamma
- Link function (g): inverse
- `\(E(y) = \mu\)`  
- `\(g(\mu)=  1/\mu\)`  
- `\(\hat \mu  = 1/(\hat a + \hat b log(z))\)`
- Residuals: `\(y - \frac{1}{\hat a + \hat b*log(z)}\)`
- `\(wpd_{norm} = Residuals\)`
]
]
]

.pull-right[

&lt;img src="figure/glm2-1.svg" width="100%" style="display: block; margin: auto;" /&gt;

]

**_ The normalized measure is chosen as residuals to make it independent of the linear predictor_**

---
# Project 2: Simulation results (GLM)

&lt;img src="figure/glm-dist-1.svg" width="90%" style="display: block; margin: auto;" /&gt;

**_Locations of distributions for different comparison look similar for higher levels, but not scale_**

---
# Project 2: Combination approach for different comparisons

&lt;img src="figure/same-scale-1.svg" width="90%" style="display: block; margin: auto;" /&gt;

.smaller[
**_The two approaches mostly overlap for higher levels and are different for smaller levels. Hence, permutation for smaller levels and model for higher level is chosen as normalization method for computational efficiency._**
]
---

# Project 2: Choosing a threshold and significant harmonies



&lt;img src="figs/threshold.png" width="70%" style="display: block; margin: auto;" /&gt;

.smaller[
.smaller[

.pull-left[
**Threshold**  
- `\(wpd_{threshold}\)` = `\(99^{th}\)` percentile of `\(wpd_{perm}\)`  
.smaller[
where, `\(wpd_{perm}=\{w_{1,perm_{1}},w_{1,perm_{2}},\dots, w_{N, permM}\}\)`
]
]
.pull-right[
**Selection criterion**    

**&lt;span style="color:black"&gt; for** `\((i \in {1, 2, \dots, N})\)`  
**&lt;span style="color:black"&gt;if** `\(w_i &gt; wpd_{threshold}\)` _select_  
**&lt;span style="color:black"&gt; else**  _reject_  
]  
  
- The threshold is defined for 99% but can be extended to define for 95% and 90% significance levels by choosing appropriate percentiles 

]
]
---

# Melbourne households example: show me the data

&lt;img src="figure/linear-scale-8new-1.svg" style="display: block; margin: auto;" /&gt;


---
#  Melbourne households example: systematic search

```r
elec %&gt;% 
*  search_gran(lowest_unit = "hour",
*              highest_unit = "month", 
*              filter_in = "wknd_wday", 
*              filter_out = "fortnight")
```


```
#&gt; [1] "hour_day"   "hour_week"  "hour_month" "day_week"   "day_month" 
#&gt; [6] "week_month" "wknd_wday"
```
&lt;br&gt;

- There are `\(^{7} P_2 = 42\)` pairs of granularities to look at looking at two at a time.
- 42 visualizations to interpret?

---
#  Melbourne households example: harmonies


```
#&gt; # A tibble: 14 x 4
#&gt;    facet_variable x_variable facet_levels x_levels
#&gt;    &lt;chr&gt;          &lt;chr&gt;             &lt;int&gt;    &lt;int&gt;
#&gt;  1 day_week       hour_day              7       24
#&gt;  2 day_month      hour_day             31       24
#&gt;  3 week_month     hour_day              5       24
#&gt;  4 wknd_wday      hour_day              2       24
#&gt;  5 hour_day       day_week             24        7
#&gt;  6 week_month     day_week              5        7
#&gt;  7 hour_day       day_month            24       31
#&gt;  8 wknd_wday      day_month             2       31
#&gt;  9 hour_day       week_month           24        5
#&gt; 10 day_week       week_month            7        5
#&gt; 11 wknd_wday      week_month            2        5
#&gt; 12 hour_day       wknd_wday            24        2
#&gt; 13 day_month      wknd_wday            31        2
#&gt; 14 week_month     wknd_wday             5        2
```

.smaller[
- only 14 out of 42 are harmonies &lt;br&gt;
- plotting other 28 pairs lead to empty combinations
]

---
#  Melbourne households example: significant harmonies

&lt;img src="figs/rank-table2.png" width="60%" style="display: block; margin: auto;" /&gt;
.smaller[
- Only 2 or 3 harmony pairs out of 14 are significant for any household 
- Significant pairs differ across households
]
---
background-image: url("figs/plot_final.png")

background-position: center
background-size: contain

---
#  Melbourne households example: compare households

&lt;img src="figs/heatplot-4.png" width="60%" style="display: block; margin: auto;" /&gt;

---

background-image: url("figs/heatplot.png")

background-position: center
background-size: contain
---
class: top left


# Compare and analyze many households?

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

## Project 3: Clustering Australian residential demand

---

# Project 3: Main contributions

- present a cluster analysis of SGSC dataset to group households with similar periodic behavior;
- cluster validation through simulations and relating to external data like census and geography [Australian Bureau of Statistics](https://www.abs.gov.au/websitedbs/censushome.nsf/home/tablebuilder) along with weather from [Bureau of Meteorology](www.bom.gov.au)

---
# Project 3: Preliminary exploration (Missingness)




&lt;img src="figure/count-gaps-1.svg" width="90%" style="display: block; margin: auto;" /&gt;
.smaller[
.smaller[
**_The missing data pattern for 100 households are shown. It looks like most missingness happens before 2013 and for a particular data in 2014._**
]
]
.smaller[
.smaller[
.smaller[
_Wang, Earo, Dianne Cook, and Rob J. Hyndman. 2020. “A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data.” Journal of Computational and Graphical Statistics_
]
]
]

---

# Project 3: Notations and Methodology

.smaller[

- `\(f_s(v)\)`: distribution of household `\(s\)` and `\(v\)` is electricity demand
- `\((A, B)\)`: harmony pair such that `\(A = \{ a_j: j = 1, 2, \dots, J\}\)` and `\(B = \{ b_k: k = 1, 2, \dots, K\}\)`
- `\(J*K\)` distributions of the form `\(f_s^{j, k}(v)\)` for each combination of categories for household `\(s\)`
- `\(wpd_{norm_s}(A,B)\)`: distance measure for the harmony pair `\((A, B)\)`
- clustering algorithm on these `\(wpd_{norm_s}(A,B)\)` instead of raw data
]
---
class:left, top

# Project 3: Implications for using probability distributions

- Dimension reduction
- Robust to outliers by trimming the tails
- Robust to missing observations
- Non-synchronized observed time periods
- Similar periodic behavior

---
.left-column[
## Timeline
### - 2020
]
.right-column[
.timeline.timeline-left.purple-flirt.timeline-with-arrows[


.timeline-block[
.arrow-right[
.timeline-content[
Mid-Candidature Review
.timeline-date[
2020/03
]]]]


.timeline-block[
.arrow-right[
.timeline-content[
Paper submitted to Journal of Computational and Graphical Statistics (JCGS)
.timeline-date[
2020/09
]]]]
]
]

---

.left-column[
## Timeline
### - 2020
### - 2021
]

.right-column[
.timeline.timeline-left.purple-flirt.timeline-with-arrows[

.timeline-block[
.arrow-right[
.timeline-content[
Paper tentatively accepted
.timeline-date[
2021/03
]]]]


.timeline-block[
.arrow-right[
.timeline-content[
Pre-submission Review 
.timeline-date[
2021/03
]]]]

.timeline-block[
.arrow-right[
.timeline-content[
Paper 2 ready for submission
.timeline-date[
2021/04
]]]]

.timeline-block[
.arrow-right[
.timeline-content[
Paper 3 ready for submission
.timeline-date[
2021/06
]]]]


.timeline-block[
.arrow-right[
.timeline-content[
Thesis submission ✌️
.timeline-date[
2021/08
]]]]
]
]

---
class: middle center

# Thank you

&lt;br&gt;
## Rob J Hyndman &amp; Dianne Cook
&lt;br&gt;
### &lt;span style="color:black"&gt; Panel members
&lt;br&gt;

### &lt;span style="color:black"&gt; &lt;small&gt; NUMBATS
### &lt;span style="color:black"&gt; &lt;small&gt; Monash Data Fluency community
### &lt;span style="color:black"&gt;&lt;small&gt; MonARCH (high performance computing cluster)



Slides created with &lt;i&gt; Rmarkdown, knitr, xaringan, xaringanthemer&lt;/i&gt; and template created by [Emi Tanaka](https://github.com/emitanaka/MBATemplate)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
