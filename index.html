<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Visualization and analysis of probability   distributions of large temporal data</title>
    <meta charset="utf-8" />
    <link href="index_files/remark-css/ninjutsu.css" rel="stylesheet" />
    <link rel="stylesheet" href="assets/animate.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-logo.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-brand.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets/styles.css" type="text/css" />
    <link rel="stylesheet" href="assets/custom.css" type="text/css" />
    <link rel="stylesheet" href="timeline.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Visualization and analysis of probability <br> distributions of large temporal data
### <br> Sayani Gupta <br>
<hr>
¬† Prof.¬†Rob J Hyndman ¬† ¬† Prof.¬†Dianne Cook ¬† ¬† Dr.¬†Peter Toscas
### <font size="5"> Department of Econometrics and Business Statistics <br> Pre-submission review <br> March 16, 2021

---










# Motivation: Electricity smart meter data

### Electricity smart meter technology &lt;br&gt; (~ 0.3 billion half hourly observations)

- Source: Department of the Environment and Energy, Australia
&lt;br&gt;
&lt;br&gt;
- Frequency:  Half hourly (interval meter reading (Kwh))
&lt;br&gt;
&lt;br&gt;
- Time Span: 2012 to 2014
&lt;br&gt;
&lt;br&gt;
- Spread: 14K (approx.) households based in Newcastle, New South Wales, and parts of Sydney
&lt;br&gt;
&lt;br&gt;

---



# Characteristics of the data

.left-column[
- missing observations
- unequal length
- different start and end date
- no behavioral pattern visible in the squeezed linear representation
- many households
]


.right-column[
&lt;img src="figure/elec-raw-1.svg" style="display: block; margin: auto;" /&gt;
]

---

class:top, center

# Theme of research




---

class:left, top

#  Projects 

.larger[
- Visualize probability distributions over different time granularities &lt;br&gt;&lt;br&gt;
- Clustering temporal data based on &lt;br&gt; probability distributions &lt;br&gt;&lt;br&gt;
- Study of Australian smart meter data
]
---




class: middle center

&lt;!-- SLIDE 4 --&gt;

.animated.bounce[
&lt;img src="images/gravitas_sticker.png" height=280px&gt;
]

## Visualizing probability distributions across bivariate cyclic temporal granularities



---

# Main contributions

 - provide a formal characterization of cyclic granularities;
 - facilitate manipulation of single- and multiple-order-up time granularities through cyclic calendar algebra;
 - develop an approach to check the feasibility of creating plots or drawing inferences for any two cyclic granularities.

---
class: top left 

# Linear to cyclic

.pull-left[
.checked[
.smaller[
- **Cyclic time granularities:** exploring different periodicities e.g. hour-of-day, day-of-month or  hour-of-week, day-of-semester
- **Multiple observations** for each level of cyclic granularity
- **summarize distribution** of measured variables
]
]
]

.pull-right[
&lt;img src="figure/linear2cyclic-1.png" style="display: block; margin: auto;" /&gt;&lt;img src="figure/linear2cyclic-2.png" style="display: block; margin: auto;" /&gt;
]

---

# Graphing distribution summary


.left-column[
- several ways to summarize a distribution
- R package ggdist for visualizing distributions and uncertainty
]

.right-column[
&lt;img src="figs/allplots.png" width="4453" style="display: block; margin: auto;" /&gt;
]


---
class: top left

# Data structure and graphical mapping

.left-column[
&lt;br&gt;
&lt;br&gt;
-  extension of tsibble data structure
-  choose any two cyclic granularities: 
`\(C_i = \{A_1, A_2, \dots, A_K\}\)` and `\(C_j = \{B_1, B_2, \dots, B_L\}\)`
- graphical mapping `\((C_i, C_j, v)\)`

- `\(^{N_C}P_2\)` displays
]

.right-column[

&lt;img src="figs/graphical_map.png" width="90%" style="display: block; margin: auto;" /&gt;

]
---
class:left, top

# Relationship of cyclic granularities

.pull-left[
**&lt;span style="color:firebrick"&gt; &lt;i&gt; Clashes&lt;/i&gt;:** pairs leading to empty sets

&lt;img src="figure/clash-1.svg" width="100%" style="display: block; margin: auto;" /&gt;

]

.pull-right[
**&lt;span style="color:firebrick"&gt; &lt;i&gt; Harmonies&lt;/i&gt;:** pairs leading to no empty sets



&lt;img src="figure/noclash-1.svg" width="100%" style="display: block; margin: auto;" /&gt;
]

Still too many harmonies for display for large `\(N_C\)`

---
class: middle center

# Show me significant harmonies only

&lt;!-- SLIDE 4 --&gt;
&lt;br&gt;
&lt;br&gt;
##  Project 2: A new metric for automatic discovery of periodic patterns in time series

---

# Project 2: Contributions

 - introduce a new distance measure for quantifying periodic interactions, which allows for identification of patterns in the time series data;
 - device a framework for choosing a threshold, which results in detection of only significantly interesting periodic patterns;
 - show that the proposed distance metric could be used to rank the periodic patterns based on how well they capture the variation in the measured variable since they have been normalized for relevant parameters.

---


# Project 2: Idea

.pull-left[
&lt;img src="figure/example-design-1.svg" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;br&gt;
- Rank harmonies `\(a (D_{null}) &lt; b (D_{var_f})\)`  
`\(&lt;  c(D_{var_x}) &lt; d(D_{var_{all}})\)`
- Gestalt theory
- within and between facet variation/distances
- compute a threshold
]

---

# Project 2: Notations
&lt;img src="figs/dist_explain.png" width="2331" style="display: block; margin: auto;" /&gt;

&lt;small&gt;
- two cyclic granularities `\(A\)` and `\(B\)` placed across x-axis and facet respectively. 
- `\(A = \{ a_j: j = 1, 2, \dots, J\}\)` and `\(B = \{ b_k: k = 1, 2, \dots, K\}\)`
- within-facet distances `\((a_{j}b_{k}, a_{j'}b_{k})\)`
- between-facet distances `\((a_{j}b_{k}, a_{j}b_{k'})\)`

---
# Project 2: Computation

## Characterizing distributions through quantiles

.small[
- `\(Q(p)=F^{-1}(p) = inf\{x: F(x) &gt;p\}\)`, where `\(F(x)\)`: distribution function, `\(Q(p)\)`: Population quantile with probability `\(0&lt;p&lt; 1\)`.
- Non-parameteric estimators : Sample quantiles
- Median-unbiased estimate of Q(p) regardless of the distribution `\(p_{(k)} = \frac{(k-1/3)}{(n+1/3)}\)`
]

.smaller[
.smaller[
.footnote[
Hyndman, Rob J., and Yanan Fan. 1996. ‚ÄúSample Quantiles in Statistical Packages.‚Äù The American Statistician 50 (4): 361‚Äì65.
]
]
]

---


# Project 2: Computation (continued)

## Make asymmetrical distributed real world observed variables more treatable

.smaller[
The empirical **Normal Quantile Transform** involves the following steps:

  1. 1) Sort the sample of measured variable `\(X\)` from the smallest to the largest observation `\(x_{(1)},\dots, x_{(i)}, .., x_{(n)}\)`.
  2. 2) Estimate the cumulative probabilities `\(p_{(1)},\dots, p_{(i)}, .., p_{(n)}\)`.
  3. 3) Transforming each observation `\(x_{(i)}\)` of `\(X\)` into observation `\(y_{(i)} = Q^{-1}(p(i))\)` of the standard normal variate `\(Y\)` , with `\(Q\)` denoting the standard normal distribution and `\(Q^{-1}\)` its inverse.
]

.smaller[
.smaller[
.smaller[
.footnote[
Bogner, K., F. Pappenberger, and H. L. Cloke. 2012. ‚ÄúTechnical Note: The Normal Quantile Transformation and Its Application in a Flood Forecasting System.‚Äù Hydrology and Earth System Sciences 16 (4): 1085‚Äì94.
]
]
]
]

---
# Project 2: Computation (continued)

## Computing distance between distributions using Jensen-Shannon distance

`$$JSD(q_1||q_2) = \frac{1}{2}D(q_1||M) + \frac{1}{2}D(q_2||M)$$`
where `\(M = \frac{q_1+q_2}{2}\)` and 
`\(D(q_1||q_2) := \int^\infty_{-\infty} q_1(x)f(\frac{q_1(x)}{q_2(x)})\)` is the KL divergence between distributions `\(q_1\)` and `\(q_2\)`.  


.smaller[
.smaller[
.smaller[
.footnote[
Bogner, K., F. Pappenberger, and H. L. Cloke. 2012. ‚ÄúTechnical Note: The Normal Quantile Transformation and Its Application in a Flood Forecasting System.‚Äù Hydrology and Earth System Sciences 16 (4): 1085‚Äì94.
]
]
]
]
---

# Project 2: Algorithm for computing `\(wpd\)`
&lt;img src="figs/algorithm1.png" width="80%" style="display: block; margin: auto;" /&gt;
---
# Project 2: Simulation study

### Description

 _ **nx** (number of x-axis categories), **nfacet** (number of facet categories), **nsim** (number of simulations to draw the distribution, **ntimes** (number of observations per combination), `\(\lambda\)` (value of tuning parameter)_
 
### Values

`\(nx = \{2, 3, 5, 7, 14, 20, 31, 50\}\)`  
`\(nfacet = \{2, 3, 5, 7, 14, 20, 31, 50\}\)`  
`\(nsim=200\)`  
`\(ntimes = 500\)`  
`\(\lambda =  0.67\)`  


---

# Project 2: Null distribution of `\(wpd\)` 

&lt;img src="figure/raw-dist-1.svg" style="display: block; margin: auto;" /&gt;

**_Distribution changes across different comparisons_**
---



# Project 2: Need to normalize

&lt;br&gt;

`\(wpd\)` values are meant to be different only when there is difference between different categories and not when 

- underlying distributions are different
(solved by using NQT)
- number of categories are different
(solved by normalizing for the number of categories)

---

# Project 2: Normalization 

### Gamma generalized linear model with inverse link

&lt;img src="figure/glm-1.svg" width="90%" style="display: block; margin: auto;" /&gt;

**_Distributions look similar for higher levels_**

---

# Project 2: Normalization 

### Permutation approach

&lt;img src="figure/perm-dist-1.svg" width="90%" style="display: block; margin: auto;" /&gt;

.smaller[
**_Location and scale of distributions look similar for different comparisons_**
]

---
# Project 2: Combination approach for different comparisons


&lt;img src="figure/same-scale-1.svg" width="90%" style="display: block; margin: auto;" /&gt;

.smaller[
**_The two approaches mostly overlap for higher levels and are different for smaller levels. Hence, permutation for smaller levels and model for higher level is chosen as normalization method for computational efficiency._**
]
---

# Project 2: Choosing a threshold and significant harmonies

.smaller[
.smaller[

1. 1) Fix harmony pair `\((A, B)\)`.

2. 2) Given the data; `\(\{v_t: t=0, 1, 2, \dots, T-1\}\)`, the `\(wpd_{norm}\)` is computed and is represented by `\(wpd_{obs}\)`.

3. 3) From the original sequence a random permutation is obtained: `\(\{v_t^*: t=0, 1, 2, \dots, T-1\}\)`

4. 4) `\(wpd_{norm}\)` is computed for the permuted sequence of the data and is represented by `\(wpd_{perm_1}\)`

5. 5) Steps (3) and (4) are repeated a large number
of times `\(M\)` (M = 200)

6. 6) For each permutation, one `\(wpd_{perm_i}\)` is obtained.   Define `\(wpd_{sample} = \{wpd_{perm_1}, wpd_{perm_2}, \dots, wpd_{perm_M}\}\)`

7. 7) Repeat Steps (1-6) for all harmony pairs `\((A, B) \in H_{N_C}\)` and store it in `\(wpd^{all}_{sample}\)`

8. 8) `\(99^{th}\)` percentiles of `\(wpd^{all}_{sample}\)` is computed and stored in `\(wpd_{threshold99}\)`

9. 9) If `\(wpd_{obs_{A, B}} &gt; wpd_{threshold99}\)`, harmony pair `\((A, B)\)` is selected with a 99% level of significance and otherwise rejected

]
]
---

# Application
.left-column[
## Melbourne households example
.checked[
* Show me the data
]
]

.right-column[
&lt;img src="figure/linear-scale-8-1.svg" style="display: block; margin: auto;" /&gt;
]

---

&lt;!-- SLIDE 9 --&gt;
.left-column[
## Melbourne households example
.checked[
* Show me the data
* search granularities `search_gran()`
]
]

.right-column[
```r
elec %&gt;% 
*  search_gran(lowest_unit = "hour",
*              highest_unit = "month", 
*              filter_in = "wknd_wday", 
*              filter_out = "fortnight")
```


```
#&gt; [1] "hour_day"   "hour_week"  "hour_month" "day_week"   "day_month" 
#&gt; [6] "week_month" "wknd_wday"
```

&lt;br&gt;
&lt;br&gt;
.smaller[
- There are `$$^{7} P_2 = 42$$` pair of granularities to look at looking at two at a time.
- 42 visualizations to interpret?

]
]
---

.left-column[
##  Melbourne households example
.checked[
* data structure
* search granularities `search_gran()`
* set of harmonies `harmony()`
]
]
.right-column[
```r
harmonies &lt;- elec %&gt;% harmony(ugran = "month",
                 filter_in = "wknd_wday",
                 filter_out = c("hhour", "fortnight")
  )
```


```
#&gt; # A tibble: 14 x 4
#&gt;    facet_variable x_variable facet_levels x_levels
#&gt;    &lt;chr&gt;          &lt;chr&gt;             &lt;int&gt;    &lt;int&gt;
#&gt;  1 day_week       hour_day              7       24
#&gt;  2 day_month      hour_day             31       24
#&gt;  3 week_month     hour_day              5       24
#&gt;  4 wknd_wday      hour_day              2       24
#&gt;  5 hour_day       day_week             24        7
#&gt;  6 week_month     day_week              5        7
#&gt;  7 hour_day       day_month            24       31
#&gt;  8 wknd_wday      day_month             2       31
#&gt;  9 hour_day       week_month           24        5
#&gt; 10 day_week       week_month            7        5
#&gt; 11 wknd_wday      week_month            2        5
#&gt; 12 hour_day       wknd_wday            24        2
#&gt; 13 day_month      wknd_wday            31        2
#&gt; 14 week_month     wknd_wday             5        2
```

.smaller[
- only 14 out 42 are harmonies &lt;br&gt;
- plotting other 28 pairs lead to empty combinations
]
]
---

# Application of Project 1 and 2 to 8 Melbourne households


&lt;img src="figure/linear-scale-8-1.svg" style="display: block; margin: auto;" /&gt;
---

# Cyclic granularities considered for analysis


# Number of displays to analyze for exhaustive exploration

There are `\(^{7}P_2 = 42\)` pair of granularities to look at placing one on the x-axis and the other on facet.
---

# Select only harmonies (16 out of 42 selected)



---
class: top left

# Select and rank significant harmonies (6 out of 16 selected)




---

class: top left

# ranking table 




# validate ranking table for household 1 

&lt;img src="figs/validate-household1.png" width="1283" style="display: block; margin: auto;" /&gt;


---
class: top left


# compare and analyze eight households
&lt;img src="figs/heatmap-8.png" width="1255" height="80%" style="display: block; margin: auto;" /&gt;

---
class: top left


# Compare and analyze many households?

&lt;br&gt;
&lt;br&gt;
## Project 3: Clustering Australian residential demand

---
class:left, top

# Clustering temporal data based on &lt;br&gt; probability distributions

## Assumptions

.smaller[
- `\(C_1: \mathbb{Z}_+ \mapsto \{L_1, L_2, L_3, \dots, L_m \}\)`
- `\(C_2: \mathbb{Z}_+ \mapsto \{L_1, L_2, L_3, \dots, L_p \}\)`
- `\(S(i, j, k)\)`: set of measurement variables corresponding to the key `\(i\)` for `\(k^{th}\)` level of `\(C_1\)` and `\(j^{th}\)` level of `\(C_2\)`
- `\(P(i, j, k)\)`: probability distribution of the measured variable corresponding to the set `\(S(i, j, k)\)`
- each observational unit `\(i\)` will have `\(mp\)` variables each of which is a probability distribution
- clustering algorithm on these `\(P(i, j, k)\)` instead of raw data
]
---
class:left, top

# Clustering temporal data based on &lt;br&gt; probability distributions

## Implications
- Dimension reduction
- Robust to outliers by trimming the tails
- Non-synchronized observed time periods
- Similar periodic behavior
- Handling autocorrelation


---
class:left, top

# Study of Australian smart meter data

.smaller[
* Provide preliminary exploratory visualization and summarisation
* Employ cluster analysis to obtain households showing similar periodic behavior and combine the findings with external data like weather conditions, socio-economic or other demographic factors of those households
* Comparative study with earlier methods which focused on clustering raw data across linear time scales as opposed to clustering probability distributions across periodic scales
]
---

.left-column[
## Timeline
### - 2019
]
.right-column[
.timeline.timeline-left.timeline-with-arrows[
.timeline-block[

.timeline-block[
.arrow-right[
.timeline-content[
Internship with Google Summer of Code
.timeline-date[
2019/08
]]]]

.timeline-block[
.arrow-right[
.timeline-content[
Young Statistician's Conference -  
won the 2nd best presentation award üèÜ
.timeline-date[
2019/10
]]]]

.timeline-block[
.arrow-right[
.timeline-content[
R package `gravitas` on CRAN
.timeline-date[
2019/11
]]]]

.timeline-block[
.arrow-right[
.timeline-content[
R package `gghdr`: graphing highest density regions at [rOpenSci](https://ropensci.org/) 
.timeline-date[
2019/12
]]]]


.timeline-block[
.arrow-right[
.timeline-content[
International Indian Statistical Association 
.timeline-date[
2019/12
]]]]

]
]
]

---

.left-column[
## Timeline
### - 2019
### - 2020
]
.right-column[
.timeline.timeline-left.purple-flirt.timeline-with-arrows[


.timeline-block[
.arrow-right[
.timeline-content[
Mid-Candidature Review
.timeline-date[
2020/03
]]]]


.timeline-block[
.arrow-right[
.timeline-content[
R package `gghdr` to be submitted on CRAN 
.timeline-date[
2020/03
]]]]


.timeline-block[
.arrow-right[
.timeline-content[
Paper to be submitted to Journal of Computational and Graphical Statistics (JCGS)
.timeline-date[
2020/04
]]]]

.timeline-block[
.arrow-right[
.timeline-content[
useR! 2020 (Tentative)
.timeline-date[
2020/07
]]]]

.timeline-block[
.arrow-right[
.timeline-content[
Compstat 2020 (Tentative)
.timeline-date[
2020/08
]]]]

.timeline-block[
.arrow-right[
.timeline-content[
R package and draft paper of Research objective-2 ready
.timeline-date[
2020/10
]]]]
]
]

---

.left-column[
## Timeline
### - 2019
### - 2020
### - 2021
]

.right-column[
.timeline.timeline-left.purple-flirt.timeline-with-arrows[

.timeline-block[
.arrow-right[
.timeline-content[
Pre-submission Review 
.timeline-date[
2021/03
]]]]

.timeline-block[
.arrow-right[
.timeline-content[
Research objective-3 draft paper ready 
.timeline-date[
2021/03
]]]]

.timeline-block[
.arrow-right[
.timeline-content[
Thesis submission ‚úåÔ∏è
.timeline-date[
2021/06
]]]]
]
]
---
class: center middle 

# Thank you

&lt;br&gt;
## Joint work with Rob J Hyndman &amp; Dianne Cook
&lt;br&gt;
&lt;br&gt;
### &lt;span style="color:black"&gt; Special thanks to NUMBATS
&lt;br&gt;
&lt;br&gt;


.footnote[
Slides created with &lt;i&gt; Rmarkdown, knitr, xaringan, xaringanthemer&lt;/i&gt;
]
---




.smaller[
.smaller[
low: less than 7 (days in a week), medium: between 7 and 14 (days in a fortnight), high: between 14 and 31 (days in a month), very high: higher than 31
]
]


# Clustering: Missing value analysis



.right-column[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="figure/count-gaps-1.svg" alt="The missing data pattern for 100 households are shown. It looks like most missingness happens before 2013 and for a particular data in 2014."  /&gt;
&lt;p class="caption"&gt;The missing data pattern for 100 households are shown. It looks like most missingness happens before 2013 and for a particular data in 2014.&lt;/p&gt;
&lt;/div&gt;
]


---







---
# Appendix

.smaller[
.smaller[
## 
`$$y = a+b*log(z) + e$$` where
`\(y = median(wpd)\)`
`\(z = nx*nfacet\)` 
`\(e_l\)` are idiosyncratic errors.  

Let `\(E(y) = \mu\)` and `\(a + b*log(z) = g(\mu)\)` where `\(g\)` is the link function.
`\(g(\mu)=  1/\mu\)` and `\(\hat \mu  = 1/(\hat a + \hat b log(z))\)`. 
`\(wpd_{glm}= (y-\hat y) = (y-1/(\hat a + \hat b log(z)))\)` 
]
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
